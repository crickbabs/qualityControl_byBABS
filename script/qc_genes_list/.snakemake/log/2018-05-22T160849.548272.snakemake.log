Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	human_globin_bed
	2

rule human_globin_bed:
    input: /camp/stp/babs/working/bahn/code/project/qc_pipeline/txt/human_globin_ids.txt
    output: /camp/stp/babs/working/bahn/code/project/qc_pipeline/bed/human_globin.bed
    jobid: 4

Error in rule human_globin_bed:
    jobid: 4
    output: /camp/stp/babs/working/bahn/code/project/qc_pipeline/bed/human_globin.bed

RuleException:
CalledProcessError in line 130 of /camp/stp/babs/working/bahn/code/project/qc_pipeline/sak/qc_pipeline.sak:
Command ' set -euo pipefail;  
		ids=$(cat /camp/stp/babs/working/bahn/code/project/qc_pipeline/txt/human_globin_ids.txt | tr "
" "|")
		echo $ids
		cat /camp/stp/babs/working/data/genomes/homo_sapiens/ensembl/GRCh38/release-86/gtf/Homo_sapiens.GRCh38.86.bed 			| grep -E $(ids::-1) 			> /camp/stp/babs/working/bahn/code/project/qc_pipeline/bed/human_globin.bed ' returned non-zero exit status 2.
  File "/camp/stp/babs/working/bahn/code/project/qc_pipeline/sak/qc_pipeline.sak", line 130, in __rule_human_globin_bed
  File "/home/camp/bahn/.conda/envs/cotidianus/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Removing output files of failed job human_globin_bed since they might be corrupted:
/camp/stp/babs/working/bahn/code/project/qc_pipeline/bed/human_globin.bed
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /camp/stp/babs/working/bahn/code/project/qc_pipeline/.snakemake/log/2018-05-22T160849.548272.snakemake.log
